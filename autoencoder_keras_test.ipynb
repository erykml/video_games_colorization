{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: gdown: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "\n",
    "# create directory for storing data\n",
    "!mkdir -p images\n",
    "\n",
    "# download zip file with training set\n",
    "!gdown https://drive.google.com/uc?id=1rTf4DJI2PoM-hgSSSzvpOQcQhui3qh-y && unzip -qq train.zip -d images\n",
    "!rm train.zip\n",
    "\n",
    "# download zip with valid set\n",
    "!gdown https://drive.google.com/uc?id=1yB-KN--FjiWZrA6XwxaSvtXhtcbk80dQ && unzip -qq valid.zip -d images\n",
    "!rm valid.zip\n",
    "\n",
    "# download zip with utils\n",
    "!gdown https://drive.google.com/uc?id=1LbwczJVpDLsMGS7yGMP7z1GCAMg4NCsS && unzip -qq utils.zip\n",
    "!rm utils.zip\n",
    "\n",
    "# change dir to the one with data \n",
    "!cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Flatten, InputLayer, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from skimage.io import imsave, imread, imshow\n",
    "from skimage.transform import resize\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 1 classes.\n",
      "Found 100 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('images/train_small/',\n",
    "                                              target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                              batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "valid_set = valid_datagen.flow_from_directory('images/valid_small/',\n",
    "                                              target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                              batch_size = BATCH_SIZE)\n",
    "\n",
    "def gen_ab_images(train_set):\n",
    "    for batch in train_set:\n",
    "        lab_batch = rgb2lab(batch[0])\n",
    "        X_batch = lab_batch[:,:,:,0] / 100\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(latent_dim))\n",
    "model.add(Dense(14*14*256))\n",
    "model.add(Reshape((14, 14, 256)))\n",
    "model.add(Conv2DTranspose(256, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2DTranspose(128, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2DTranspose(2, (3, 3), activation='relu', padding='same', strides=2))       \n",
    "model.compile(optimizer='rmsprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 111, 111, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 56, 56, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 256)       295168    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               12845312  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50176)             12895232  \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 28, 28, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 56, 56, 128)       295040    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 112, 112, 64)      73792     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 224, 224, 2)       1154      \n",
      "=================================================================\n",
      "Total params: 27,088,450\n",
      "Trainable params: 27,088,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"output/first_run\")\n",
    "model.fit(x=gen_ab_images(train_set), \n",
    "          callbacks=[tensorboard], \n",
    "          epochs=N_EPOCHS, \n",
    "          validation_data=gen_ab_images(valid_set), \n",
    "          steps_per_epoch=len(train_set),\n",
    "          validation_steps=len(valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batch = next(gen_ab_images(valid_set))\n",
    "Xtest, Ytest = batch[0], batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 85ms/step\n",
      "0.01884119212627411\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "output = model.predict(Xtest)\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "i = randrange(len(output))\n",
    "cur = np.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "cur[:,:,0] = Xtest[i][:,:,0] * 100\n",
    "cur[:,:,1:] = output[i]\n",
    "cur = lab2rgb(cur)\n",
    "imshow(cur)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
